{
  "scenario_id": "16-disaster-recovery-rebuild",
  "difficulty": "hard",
  "duration": "45 min",
  "commands": [
    {
      "name": "Step 1: Review the DR-Ready Backend Configuration",
      "command": "cat << 'TFEOF'\n# backend.tf - Disaster Recovery Ready State Backend\n# This configuration creates a production-grade state backend with:\n# - Cross-region replication for state files\n# - Versioning for state history\n# - State locking with DynamoDB\n# - Automated backup retention\n\nterraform {\n  backend \"s3\" {\n    bucket         = \"k8s-simulator-tfstate-primary\"\n    key            = \"prod/infrastructure.tfstate\"\n    region         = \"us-east-1\"\n    encrypt        = true\n    kms_key_id     = \"arn:aws:kms:us-east-1:123456789012:key/abc-123\"\n    dynamodb_table = \"terraform-state-lock\"\n    \n    # S3 bucket should have:\n    # - Versioning enabled\n    # - Cross-region replication to us-west-2\n    # - Lifecycle policies for old versions\n    # - MFA delete protection\n  }\n}\n\n# DynamoDB table for state locking (must exist before init)\nresource \"aws_dynamodb_table\" \"terraform_locks\" {\n  name         = \"terraform-state-lock\"\n  billing_mode = \"PAY_PER_REQUEST\"\n  hash_key     = \"LockID\"\n\n  attribute {\n    name = \"LockID\"\n    type = \"S\"\n  }\n\n  point_in_time_recovery {\n    enabled = true\n  }\n\n  tags = {\n    Name        = \"Terraform State Lock Table\"\n    Environment = \"prod\"\n    Critical    = \"true\"\n  }\n}\nTFEOF",
      "description": "Examine a disaster recovery ready backend configuration with cross-region replication",
      "explanation": "A production-grade DR setup requires multiple layers of protection. The S3 backend uses encryption at rest with KMS, versioning to maintain state history, and cross-region replication to protect against regional failures. DynamoDB provides distributed state locking to prevent concurrent modifications. Point-in-time recovery on DynamoDB ensures you can restore the lock table if corrupted. MFA delete on the S3 bucket prevents accidental deletion of state files.",
      "what_it_does": "Shows a complete backend configuration with encryption, versioning, cross-region replication, and state locking for disaster recovery.",
      "next_step": "Next we'll create the replicated state bucket infrastructure.",
      "cleanup": false
    },
    {
      "name": "Step 2: Create the State Bucket with Replication",
      "command": "cat << 'TFEOF'\n# state_backend_dr.tf - State Backend with Cross-Region Replication\n\n# Primary state bucket in us-east-1\nresource \"aws_s3_bucket\" \"tfstate_primary\" {\n  bucket = \"k8s-simulator-tfstate-primary\"\n  \n  tags = {\n    Name        = \"Terraform State Primary\"\n    Environment = \"prod\"\n    Region      = \"primary\"\n  }\n}\n\n# Enable versioning on primary bucket\nresource \"aws_s3_bucket_versioning\" \"tfstate_primary\" {\n  bucket = aws_s3_bucket.tfstate_primary.id\n  \n  versioning_configuration {\n    status     = \"Enabled\"\n    mfa_delete = \"Disabled\"  # In real prod: \"Enabled\"\n  }\n}\n\n# Replica bucket in us-west-2 for disaster recovery\nresource \"aws_s3_bucket\" \"tfstate_replica\" {\n  provider = aws.west\n  bucket   = \"k8s-simulator-tfstate-replica\"\n  \n  tags = {\n    Name        = \"Terraform State Replica\"\n    Environment = \"prod\"\n    Region      = \"replica\"\n  }\n}\n\n# Enable versioning on replica bucket\nresource \"aws_s3_bucket_versioning\" \"tfstate_replica\" {\n  provider = aws.west\n  bucket   = aws_s3_bucket.tfstate_replica.id\n  \n  versioning_configuration {\n    status = \"Enabled\"\n  }\n}\n\n# IAM role for replication\nresource \"aws_iam_role\" \"replication\" {\n  name = \"s3-tfstate-replication-role\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [{\n      Action = \"sts:AssumeRole\"\n      Effect = \"Allow\"\n      Principal = {\n        Service = \"s3.amazonaws.com\"\n      }\n    }]\n  })\n}\n\n# Replication configuration\nresource \"aws_s3_bucket_replication_configuration\" \"tfstate\" {\n  depends_on = [aws_s3_bucket_versioning.tfstate_primary]\n  \n  role   = aws_iam_role.replication.arn\n  bucket = aws_s3_bucket.tfstate_primary.id\n\n  rule {\n    id     = \"replicate-tfstate\"\n    status = \"Enabled\"\n\n    destination {\n      bucket        = aws_s3_bucket.tfstate_replica.arn\n      storage_class = \"STANDARD_IA\"\n      \n      replication_time {\n        status = \"Enabled\"\n        time {\n          minutes = 15\n        }\n      }\n      \n      metrics {\n        status = \"Enabled\"\n        event_threshold {\n          minutes = 15\n        }\n      }\n    }\n  }\n}\nTFEOF",
      "description": "Create S3 buckets with cross-region replication for state file redundancy",
      "explanation": "Cross-region replication ensures your state file is automatically copied to a secondary region within 15 minutes. If the primary region (us-east-1) becomes unavailable, you can reconfigure your backend to use the replica bucket in us-west-2. Versioning on both buckets means you have a complete history of state changes. The STANDARD_IA storage class on replicas reduces costs while maintaining quick access during DR scenarios. Replication time control guarantees predictable replication windows.",
      "what_it_does": "Creates primary and replica S3 buckets with versioning and replication configuration for state file disaster recovery.",
      "next_step": "Now let's simulate a state backup procedure.",
      "cleanup": false
    },
    {
      "name": "Step 3: Manual State Backup Procedure",
      "command": "echo '=== Manual State Backup Script ==='\necho ''\necho '#!/bin/bash'\necho '# backup_tfstate.sh - Manual state backup procedure'\necho ''\necho 'BACKUP_DATE=$(date +%Y%m%d_%H%M%S)'\necho 'STATE_BUCKET=\"k8s-simulator-tfstate-primary\"'\necho 'STATE_KEY=\"prod/infrastructure.tfstate\"'\necho 'BACKUP_BUCKET=\"k8s-simulator-tfstate-backups\"'\necho 'LOCAL_BACKUP=\"./backups/terraform_${BACKUP_DATE}.tfstate\"'\necho ''\necho '# Step 1: Pull current state from S3'\necho 'aws s3 cp s3://${STATE_BUCKET}/${STATE_KEY} ${LOCAL_BACKUP}'\necho ''\necho '# Step 2: Verify the backup file integrity'\necho 'if [ -f \"${LOCAL_BACKUP}\" ]; then'\necho '  jq empty ${LOCAL_BACKUP} 2>/dev/null'\necho '  if [ $? -eq 0 ]; then'\necho '    echo \"✓ State file is valid JSON\"'\necho '  else'\necho '    echo \"✗ State file is corrupted!\"'\necho '    exit 1'\necho '  fi'\necho 'fi'\necho ''\necho '# Step 3: Upload to backup bucket with metadata'\necho 'aws s3 cp ${LOCAL_BACKUP} \\\\'\necho '  s3://${BACKUP_BUCKET}/manual_backups/$(basename ${LOCAL_BACKUP}) \\\\'\necho '  --metadata \"backup-type=manual,timestamp=${BACKUP_DATE}\"'\necho ''\necho '# Step 4: List recent versions in primary bucket'\necho 'aws s3api list-object-versions \\\\'\necho '  --bucket ${STATE_BUCKET} \\\\'\necho '  --prefix ${STATE_KEY} \\\\'\necho '  --max-items 5'\necho ''\necho 'echo \"Backup complete: ${LOCAL_BACKUP}\"'\necho ''\necho '=== Why Manual Backups Matter ==='\necho '- Automated backups might fail silently'\necho '- Pre-deployment backups allow quick rollback'\necho '- Offline backups protect against account compromise'\necho '- Version history in S3 might have retention limits'",
      "description": "Learn how to manually backup Terraform state files before critical operations",
      "explanation": "Manual backups are essential before major infrastructure changes. Even with S3 versioning and replication, you should create explicit backups before deployments. This script pulls the current state, validates it's valid JSON (corrupted states can break everything), and stores it with metadata. The metadata makes it easy to find backups later. Always verify backup integrity immediately -- a corrupted backup is useless during recovery. In CI/CD pipelines, this runs automatically before every terraform apply.",
      "what_it_does": "Shows a complete bash script for manually backing up state files with validation and metadata tagging.",
      "next_step": "Let's simulate a disaster scenario and recover from it.",
      "cleanup": false
    },
    {
      "name": "Step 4: Simulate State File Corruption",
      "command": "echo '=== DISASTER SCENARIO: State File Corrupted ==='\necho ''\necho 'Situation: After a failed deployment, your state file is corrupted.'\necho ''\necho '$ terraform plan'\necho 'Error: Failed to load state'\necho ''\necho 'Error: state snapshot was created by Terraform v1.6.0, which is newer than'\necho 'current v1.5.0; upgrade to Terraform v1.6.0 or greater to work with this state'\necho ''\necho '--- OR ---'\necho ''\necho 'Error: Failed to read state: unexpected end of JSON input'\necho ''\necho '=== Diagnosis Commands ==='\necho ''\necho '# Check local state file (if using local backend)'\necho '$ cat terraform.tfstate | jq empty'\necho 'parse error: Expected separator between values at line 234, column 5'\necho ''\necho '# Check S3 state file size'\necho '$ aws s3 ls s3://k8s-simulator-tfstate-primary/prod/infrastructure.tfstate --recursive --human-readable'\necho '2024-02-09 14:32:15    0 Bytes prod/infrastructure.tfstate  # <- CORRUPTED!'\necho ''\necho '# List available versions in S3'\necho '$ aws s3api list-object-versions \\\\'\necho '    --bucket k8s-simulator-tfstate-primary \\\\'\necho '    --prefix prod/infrastructure.tfstate'\necho ''\necho 'Versions:'\necho '  - VersionId: xyz789 (0 bytes) - CURRENT - CORRUPTED'\necho '  - VersionId: abc456 (45.2 KB) - 2024-02-09 14:30:12 - GOOD'\necho '  - VersionId: def123 (44.8 KB) - 2024-02-09 12:15:33 - GOOD'\necho ''\necho '=== Recovery Options ==='\necho '1. Restore from S3 version history (fastest)'\necho '2. Restore from manual backup'\necho '3. Restore from replica bucket'\necho '4. Rebuild state using terraform import (last resort)'",
      "description": "Understand common state corruption scenarios and initial diagnosis steps",
      "explanation": "State corruption can happen due to failed writes, version mismatches, or network interruptions during state saves. The symptoms vary: parse errors, empty files, or version incompatibility. Always diagnose first -- check if it's local corruption, S3 corruption, or a version mismatch. S3 versioning is your first line of defense because you can list previous versions and restore the last known-good state. Manual backups are your second layer. The replica bucket is your DR failover. Only use terraform import as a last resort because it's labor-intensive.",
      "what_it_does": "Simulates a state corruption scenario with diagnosis commands showing how to identify the issue and available recovery options.",
      "next_step": "Now let's restore from S3 version history.",
      "cleanup": false
    },
    {
      "name": "Step 5: Restore State from S3 Version History",
      "command": "echo '=== Recovery Method 1: S3 Version Restore ==='\necho ''\necho '# Step 1: Identify the last good version'\necho '$ aws s3api list-object-versions \\\\'\necho '    --bucket k8s-simulator-tfstate-primary \\\\'\necho '    --prefix prod/infrastructure.tfstate \\\\'\necho '    --query \"Versions[?Size>\\`0\\`] | [0:5].{VersionId:VersionId, Size:Size, LastModified:LastModified}\"'\necho ''\necho '[\n  {\n    \"VersionId\": \"abc456xyz\",\n    \"Size\": 46234,\n    \"LastModified\": \"2024-02-09T14:30:12.000Z\"\n  },\n  {\n    \"VersionId\": \"def123uvw\",\n    \"Size\": 45899,\n    \"LastModified\": \"2024-02-09T12:15:33.000Z\"\n  }\n]'\necho ''\necho '# Step 2: Download the last good version locally'\necho '$ aws s3api get-object \\\\'\necho '    --bucket k8s-simulator-tfstate-primary \\\\'\necho '    --key prod/infrastructure.tfstate \\\\'\necho '    --version-id abc456xyz \\\\'\necho '    terraform.tfstate.recovered'\necho ''\necho 'Downloaded: terraform.tfstate.recovered (46234 bytes)'\necho ''\necho '# Step 3: Validate the recovered state'\necho '$ jq . terraform.tfstate.recovered > /dev/null && echo \"Valid JSON\"'\necho 'Valid JSON'\necho ''\necho '$ jq -r .terraform_version terraform.tfstate.recovered'\necho '1.5.7'\necho ''\necho '# Step 4: Restore to S3 (overwrites current)'\necho '$ aws s3 cp terraform.tfstate.recovered \\\\'\necho '    s3://k8s-simulator-tfstate-primary/prod/infrastructure.tfstate'\necho ''\necho 'upload: terraform.tfstate.recovered to s3://...''\necho ''\necho '# Step 5: Verify recovery'\necho '$ terraform init -reconfigure'\necho 'Backend configuration reinitialized.'\necho ''\necho '$ terraform plan'\necho 'No changes. Your infrastructure matches the configuration.'\necho ''\necho '✓ State successfully restored from version abc456xyz'",
      "description": "Restore a corrupted state file from S3 version history",
      "explanation": "S3 versioning is the fastest recovery method. You list versions, filter out the corrupted one (size=0), download the most recent good version, validate it's valid JSON and compatible with your Terraform version, then upload it back. The upload creates a new version, so you're not overwriting anything permanently. After restore, always run terraform plan to verify the recovered state matches reality. If there are differences, it might mean infrastructure changed while the state was corrupted. This entire recovery process can be completed in under 5 minutes.",
      "what_it_does": "Demonstrates the complete process of restoring state from S3 version history with validation steps.",
      "next_step": "Let's see how to recover from a regional failure using the replica.",
      "cleanup": false
    },
    {
      "name": "Step 6: Failover to Replica Bucket (Regional DR)",
      "command": "echo '=== Disaster Scenario: Primary Region Down ==='\necho ''\necho 'Situation: AWS us-east-1 region is experiencing an outage.'\necho 'Your primary state bucket is unreachable.'\necho ''\necho '$ terraform plan'\necho 'Error: Error loading state: RequestError: send request failed'\necho 'caused by: Get \"https://k8s-simulator-tfstate-primary.s3.us-east-1.amazonaws.com/...\"'\necho 'dial tcp: lookup k8s-simulator-tfstate-primary.s3.us-east-1.amazonaws.com: no such host'\necho ''\necho '=== Failover Procedure ==='\necho ''\necho '# Step 1: Create a backend override configuration'\necho '$ cat > backend_override.tf << EOF'\necho 'terraform {'\necho '  backend \"s3\" {'\necho '    bucket         = \"k8s-simulator-tfstate-replica\"  # Changed to replica'\necho '    key            = \"prod/infrastructure.tfstate\"'\necho '    region         = \"us-west-2\"                      # Changed to replica region'\necho '    encrypt        = true'\necho '    dynamodb_table = \"terraform-state-lock-west\"     # West region lock table'\necho '  }'\necho '}'\necho 'EOF'\necho ''\necho '# Step 2: Reinitialize with the replica backend'\necho '$ terraform init -migrate-state -reconfigure'\necho ''\necho 'Initializing the backend...'\necho 'Acquiring state lock. This may take a few moments...'\necho 'Do you want to migrate all workspaces to \"s3\"?'\necho '  Terraform will copy the following state data from the previous backend:'\necho '  - workspace \"default\"'\necho ''\necho 'Enter a value: yes'\necho ''\necho 'Successfully configured the backend \"s3\"!'\necho ''\necho '# Step 3: Verify state is accessible'\necho '$ terraform state list'\necho 'aws_dynamodb_table.terraform_locks'\necho 'aws_s3_bucket.tfstate_primary'\necho 'aws_s3_bucket.tfstate_replica'\necho 'aws_s3_bucket_replication_configuration.tfstate'\necho '...'\necho ''\necho '# Step 4: Document the failover'\necho 'echo \"$(date): Failed over to replica in us-west-2 due to us-east-1 outage\" >> DR.log'\necho ''\necho '✓ Now operating on replica bucket in us-west-2'\necho '✓ Continue normal Terraform operations'\necho '✓ When primary region recovers, reverse the process'",
      "description": "Failover to a replica state bucket during a regional outage",
      "explanation": "Regional failures are rare but catastrophic if you're not prepared. With cross-region replication, your replica bucket is only 15 minutes behind the primary. During failover, you create a backend override file pointing to the replica, then run init with -migrate-state. Terraform copies the state to the new backend (even though it's already there via replication, this ensures consistency). You also need a DynamoDB lock table in the replica region. After failover, all terraform operations work normally against the replica. When the primary region recovers, you reverse the process to fail back.",
      "what_it_does": "Shows the complete failover process to a replica bucket during a regional AWS outage with backend reconfiguration.",
      "next_step": "Now let's learn how to import existing resources if state is lost.",
      "cleanup": false
    },
    {
      "name": "Step 7: Rebuild State Using Terraform Import",
      "command": "echo '=== Last Resort: Rebuilding State with Import ==='\necho ''\necho 'Scenario: All backups failed. State is completely lost.'\necho 'But your infrastructure still exists in AWS.'\necho ''\necho '=== Step 1: Create minimal config for existing resources ==='\necho ''\necho '$ cat > recovery.tf << EOF'\necho '# Declare resources that exist in AWS but are missing from state'\necho 'resource \"aws_s3_bucket\" \"tfstate_primary\" {'\necho '  bucket = \"k8s-simulator-tfstate-primary\"'\necho '}'\necho ''\necho 'resource \"aws_dynamodb_table\" \"terraform_locks\" {'\necho '  name         = \"terraform-state-lock\"'\necho '  billing_mode = \"PAY_PER_REQUEST\"'\necho '  hash_key     = \"LockID\"'\necho '  '\necho '  attribute {'\necho '    name = \"LockID\"'\necho '    type = \"S\"'\necho '  }'\necho '}'\necho 'EOF'\necho ''\necho '=== Step 2: Import existing resources ==='\necho ''\necho '$ terraform import aws_s3_bucket.tfstate_primary k8s-simulator-tfstate-primary'\necho ''\necho 'aws_s3_bucket.tfstate_primary: Importing from ID \"k8s-simulator-tfstate-primary\"...'\necho 'aws_s3_bucket.tfstate_primary: Import prepared!'\necho '  Prepared aws_s3_bucket for import'\necho 'aws_s3_bucket.tfstate_primary: Refreshing state...'\necho ''\necho 'Import successful!'\necho ''\necho '$ terraform import aws_dynamodb_table.terraform_locks terraform-state-lock'\necho ''\necho 'aws_dynamodb_table.terraform_locks: Importing from ID \"terraform-state-lock\"...'\necho 'aws_dynamodb_table.terraform_locks: Import prepared!'\necho 'aws_dynamodb_table.terraform_locks: Refreshing state...'\necho ''\necho 'Import successful!'\necho ''\necho '=== Step 3: Verify the imported state ==='\necho ''\necho '$ terraform plan'\necho ''\necho 'aws_s3_bucket.tfstate_primary: Refreshing state...'\necho 'aws_dynamodb_table.terraform_locks: Refreshing state...'\necho ''\necho 'Note: Objects have changed outside of Terraform'\necho ''\necho 'Terraform detected the following changes made outside of Terraform:'\necho ''\necho '  # aws_s3_bucket.tfstate_primary has changed'\necho '  ~ resource \"aws_s3_bucket\" \"tfstate_primary\" {'\necho '        id     = \"k8s-simulator-tfstate-primary\"'\necho '      + tags   = {'\necho '          + Environment = \"prod\"'\necho '        }'\necho '    }'\necho ''\necho 'Plan: 0 to add, 1 to change, 0 to destroy.'\necho ''\necho '=== Key Points ==='\necho '- Import brings existing infrastructure into Terraform management'\necho '- You must manually write resource blocks that match AWS reality'\necho '- Use terraform plan to see drift between config and reality'\necho '- Import is time-consuming for large infrastructures (hundreds of resources)'\necho '- Consider tools like Terraformer to automate bulk imports'",
      "description": "Import existing AWS resources into a new state file when state is completely lost",
      "explanation": "Import is the disaster recovery method of last resort. You manually write resource blocks for each existing resource, then use terraform import to link them to Terraform state. The import reads the current configuration from AWS and stores it in state. However, import doesn't generate configuration files -- you must write those manually, matching AWS reality exactly. This is why backups are so important: importing hundreds of resources is tedious and error-prone. Tools like Terraformer can automate this, but they're not perfect. After importing, always run plan to catch drift.",
      "what_it_does": "Demonstrates importing existing AWS resources into Terraform state when the state file is completely lost.",
      "next_step": "Let's look at automated DR testing procedures.",
      "cleanup": false
    },
    {
      "name": "Step 8: Automated DR Testing Script",
      "command": "echo '=== Automated DR Test Procedure ==='\necho ''\necho '#!/bin/bash'\necho '# dr_test.sh - Automated disaster recovery testing'\necho '# Run this monthly to verify DR procedures work'\necho ''\necho 'set -euo pipefail'\necho ''\necho 'PRIMARY_BUCKET=\"k8s-simulator-tfstate-primary\"'\necho 'REPLICA_BUCKET=\"k8s-simulator-tfstate-replica\"'\necho 'TEST_DATE=$(date +%Y%m%d_%H%M%S)'\necho 'TEST_RESULTS=\"dr_test_${TEST_DATE}.log\"'\necho ''\necho '# Test 1: Verify S3 replication status'\necho 'echo \"[TEST 1] Checking replication status...\" | tee -a ${TEST_RESULTS}'\necho 'REPLICATION_STATUS=$(aws s3api get-bucket-replication \\\\'\necho '  --bucket ${PRIMARY_BUCKET} \\\\'\necho '  --query \"ReplicationConfiguration.Rules[0].Status\" \\\\'\necho '  --output text)'\necho ''\necho 'if [ \"$REPLICATION_STATUS\" = \"Enabled\" ]; then'\necho '  echo \"✓ Replication is enabled\" | tee -a ${TEST_RESULTS}'\necho 'else'\necho '  echo \"✗ Replication is NOT enabled!\" | tee -a ${TEST_RESULTS}'\necho '  exit 1'\necho 'fi'\necho ''\necho '# Test 2: Verify state file exists in both regions'\necho 'echo \"[TEST 2] Verifying state files exist...\" | tee -a ${TEST_RESULTS}'\necho 'aws s3 ls s3://${PRIMARY_BUCKET}/prod/infrastructure.tfstate || exit 1'\necho 'aws s3 ls s3://${REPLICA_BUCKET}/prod/infrastructure.tfstate || exit 1'\necho 'echo \"✓ State files exist in both regions\" | tee -a ${TEST_RESULTS}'\necho ''\necho '# Test 3: Compare state file checksums'\necho 'echo \"[TEST 3] Comparing primary and replica checksums...\" | tee -a ${TEST_RESULTS}'\necho 'PRIMARY_ETAG=$(aws s3api head-object \\\\'\necho '  --bucket ${PRIMARY_BUCKET} \\\\'\necho '  --key prod/infrastructure.tfstate \\\\'\necho '  --query ETag --output text)'\necho ''\necho 'REPLICA_ETAG=$(aws s3api head-object \\\\'\necho '  --bucket ${REPLICA_BUCKET} \\\\'\necho '  --key prod/infrastructure.tfstate \\\\'\necho '  --query ETag --output text)'\necho ''\necho 'if [ \"$PRIMARY_ETAG\" = \"$REPLICA_ETAG\" ]; then'\necho '  echo \"✓ Checksums match\" | tee -a ${TEST_RESULTS}'\necho 'else'\necho '  echo \"⚠ Checksums differ (replication lag?)\" | tee -a ${TEST_RESULTS}'\necho 'fi'\necho ''\necho '# Test 4: Verify DynamoDB lock tables'\necho 'echo \"[TEST 4] Checking state lock tables...\" | tee -a ${TEST_RESULTS}'\necho 'aws dynamodb describe-table --table-name terraform-state-lock --region us-east-1 > /dev/null'\necho 'aws dynamodb describe-table --table-name terraform-state-lock-west --region us-west-2 > /dev/null'\necho 'echo \"✓ Lock tables exist in both regions\" | tee -a ${TEST_RESULTS}'\necho ''\necho '# Test 5: Test restore from version history'\necho 'echo \"[TEST 5] Testing version restore...\" | tee -a ${TEST_RESULTS}'\necho 'LATEST_VERSION=$(aws s3api list-object-versions \\\\'\necho '  --bucket ${PRIMARY_BUCKET} \\\\'\necho '  --prefix prod/infrastructure.tfstate \\\\'\necho '  --query \"Versions[0].VersionId\" \\\\'\necho '  --output text)'\necho ''\necho 'aws s3api get-object \\\\'\necho '  --bucket ${PRIMARY_BUCKET} \\\\'\necho '  --key prod/infrastructure.tfstate \\\\'\necho '  --version-id ${LATEST_VERSION} \\\\'\necho '  /tmp/test_restore.tfstate > /dev/null'\necho ''\necho 'if jq empty /tmp/test_restore.tfstate 2>/dev/null; then'\necho '  echo \"✓ Successfully restored and validated state from version history\" | tee -a ${TEST_RESULTS}'\necho '  rm /tmp/test_restore.tfstate'\necho 'else'\necho '  echo \"✗ Restored state is corrupted!\" | tee -a ${TEST_RESULTS}'\necho '  exit 1'\necho 'fi'\necho ''\necho 'echo \"\" | tee -a ${TEST_RESULTS}'\necho 'echo \"=== DR Test Complete ===\" | tee -a ${TEST_RESULTS}'\necho 'echo \"All tests passed on ${TEST_DATE}\" | tee -a ${TEST_RESULTS}'\necho 'echo \"Next test due: $(date -d \"+30 days\" +%Y-%m-%d)\" | tee -a ${TEST_RESULTS}'",
      "description": "Automated script to test disaster recovery procedures monthly",
      "explanation": "DR procedures must be tested regularly, or they won't work when you need them. This script automates five critical tests: replication is enabled, state files exist in both regions, checksums match (within replication lag), lock tables exist, and version restoration works. Run this monthly in CI/CD. If any test fails, you know before a real disaster strikes. Testing DR procedures is often overlooked, but it's as important as having them. Document test results for compliance audits. Many regulatory frameworks (SOC2, ISO 27001) require periodic DR testing.",
      "what_it_does": "Shows a comprehensive bash script for automated monthly testing of disaster recovery procedures with validation checks.",
      "next_step": "Let's review DR best practices and runbook creation.",
      "cleanup": false
    },
    {
      "name": "Step 9: DR Runbook and Best Practices",
      "command": "cat << 'RUNBOOKEOF'\n=== Terraform Disaster Recovery Runbook ===\n\nPURPOSE:\nProvide step-by-step procedures for recovering from Terraform state disasters.\n\n--- SCENARIO 1: State File Corrupted ---\n\nSYMPTOMS:\n- terraform plan fails with JSON parse errors\n- State file size is 0 bytes\n- Version mismatch errors\n\nRECOVERY PROCEDURE:\n1. Do NOT run terraform apply\n2. List S3 versions: aws s3api list-object-versions --bucket <bucket> --prefix <key>\n3. Identify last good version (non-zero size)\n4. Download: aws s3api get-object --version-id <id> ...\n5. Validate: jq empty <file>\n6. Restore: aws s3 cp <file> s3://<bucket>/<key>\n7. Test: terraform plan\n\nTIME ESTIMATE: 5-10 minutes\nDOWNTIME: None (read-only operations)\n\n--- SCENARIO 2: Regional Outage (Primary Region Down) ---\n\nSYMPTOMS:\n- Cannot reach S3 bucket in primary region\n- Terraform init fails with network errors\n- AWS Service Health Dashboard shows region issues\n\nRECOVERY PROCEDURE:\n1. Confirm primary region is down (AWS Status Page)\n2. Create backend_override.tf pointing to replica bucket\n3. Run: terraform init -migrate-state -reconfigure\n4. Confirm migration\n5. Test: terraform plan\n6. Document failover in incident log\n7. Update team via Slack/PagerDuty\n\nTIME ESTIMATE: 15-20 minutes\nDOWNTIME: None (existing infra unaffected)\n\nFAILBACK PROCEDURE:\n1. Confirm primary region is healthy\n2. Restore original backend configuration\n3. Run: terraform init -migrate-state -reconfigure\n4. Test: terraform plan\n5. Document failback\n\n--- SCENARIO 3: Complete State Loss ---\n\nSYMPTOMS:\n- All backups failed or deleted\n- S3 versioning was disabled\n- State file never existed\n\nRECOVERY PROCEDURE:\n1. Take inventory of existing AWS resources\n2. Create resource blocks for each resource\n3. Import resources: terraform import <address> <id>\n4. Run terraform plan to check for drift\n5. Apply any necessary corrections\n6. IMMEDIATELY set up proper backups\n\nTIME ESTIMATE: Hours to days (depending on resource count)\nDOWNTIME: Potential for drift-related issues\n\n--- BEST PRACTICES ---\n\n1. PREVENTION:\n   ✓ Enable S3 versioning on state bucket\n   ✓ Configure cross-region replication\n   ✓ Use DynamoDB for state locking\n   ✓ Enable MFA delete on state bucket\n   ✓ Encrypt state with KMS\n   ✓ Run automated backups before deployments\n   ✓ Set up lifecycle policies (retain 90 days of versions)\n\n2. DETECTION:\n   ✓ Monitor S3 replication lag (CloudWatch)\n   ✓ Alert on state file size = 0\n   ✓ Alert on failed terraform operations\n   ✓ Monthly DR tests\n\n3. DOCUMENTATION:\n   ✓ Keep this runbook updated\n   ✓ Document all state migrations\n   ✓ Maintain inventory of critical resources\n   ✓ Record backup locations and retention\n\n4. TEAM PREPAREDNESS:\n   ✓ Train all engineers on DR procedures\n   ✓ Assign DR roles (primary/secondary responders)\n   ✓ Practice DR drills quarterly\n   ✓ Keep runbook accessible during outages\n\n5. ESCALATION:\n   Level 1: State corruption -> Restore from S3 versions\n   Level 2: Regional outage -> Failover to replica\n   Level 3: Complete loss -> Import existing resources\n   Level 4: Import fails -> Contact HashiCorp support + rebuild from scratch\n\n--- CONTACTS ---\n\nPrimary On-Call: DevOps team (PagerDuty)\nSecondary: Platform Engineering team\nHashiCorp Support: support.hashicorp.com (Enterprise only)\nAWS Support: AWS Console > Support Center\n\n--- POST-INCIDENT ---\n\nAfter any DR event:\n1. Write incident post-mortem\n2. Update runbook with lessons learned\n3. Add automated checks to prevent recurrence\n4. Schedule team review meeting\n5. Test the fix in staging\n\nRUNBOOKEOF",
      "description": "Review a complete disaster recovery runbook with procedures and best practices",
      "explanation": "A DR runbook is a living document that guides your response during high-stress incidents. It should be detailed enough that someone unfamiliar with your infrastructure can execute the steps. Include symptoms for each scenario so responders can quickly identify which procedure to follow. Time estimates help with incident response planning. Document escalation paths and contacts. Most importantly, update the runbook after every incident with lessons learned. Store the runbook in multiple places (wiki, printed copy, S3) so it's accessible even during outages.",
      "what_it_does": "Presents a comprehensive disaster recovery runbook covering common scenarios, procedures, best practices, and escalation paths.",
      "next_step": "Finally, let's review key metrics for DR readiness.",
      "cleanup": false
    },
    {
      "name": "Step 10: DR Metrics and Monitoring",
      "command": "echo '=== Key Disaster Recovery Metrics ==='\necho ''\necho '1. RTO (Recovery Time Objective)'\necho '   Definition: Maximum acceptable downtime'\necho '   '\necho '   Scenario 1 (State corruption): RTO = 10 minutes'\necho '   Scenario 2 (Regional failover): RTO = 20 minutes'\necho '   Scenario 3 (State rebuild): RTO = 24 hours'\necho ''\necho '2. RPO (Recovery Point Objective)'\necho '   Definition: Maximum acceptable data loss'\necho '   '\necho '   With S3 replication (15 min lag): RPO = 15 minutes'\necho '   With S3 versioning only: RPO = 0 (point-in-time recovery)'\necho '   Without either: RPO = last manual backup'\necho ''\necho '3. Replication Lag (Real-time Monitoring)'\necho '   '\necho '   CloudWatch Metric: s3:ReplicationLatency'\necho '   Alert threshold: > 30 minutes'\necho '   '\necho '   $ aws cloudwatch put-metric-alarm \\\\'\necho '     --alarm-name tfstate-replication-lag \\\\'\necho '     --alarm-description \"Alert when state replication lags\" \\\\'\necho '     --metric-name ReplicationLatency \\\\'\necho '     --namespace AWS/S3 \\\\'\necho '     --statistic Maximum \\\\'\necho '     --period 300 \\\\'\necho '     --threshold 1800 \\\\'\necho '     --comparison-operator GreaterThanThreshold'\necho ''\necho '4. State File Size Monitoring'\necho '   '\necho '   CloudWatch Metric: Custom metric'\necho '   Alert: State file size = 0 bytes (corruption)'\necho '   '\necho '   $ aws cloudwatch put-metric-data \\\\'\necho '     --namespace TerraformState \\\\'\necho '     --metric-name StateFileSize \\\\'\necho '     --value $(aws s3 ls s3://bucket/key --summarize | grep \"Total Size\" | awk \"{print \\$3}\") \\\\'\necho '     --unit Bytes'\necho ''\necho '5. Backup Success Rate'\necho '   '\necho '   Target: 100% of pre-deployment backups succeed'\necho '   Measure: Count(successful backups) / Count(deployments)'\necho '   Alert: Any backup failure'\necho ''\necho '6. DR Test Frequency'\necho '   '\necho '   Target: Monthly DR tests with 100% pass rate'\necho '   Measure: Days since last successful DR test'\necho '   Alert: > 35 days since last test'\necho ''\necho '7. Version Retention Compliance'\necho '   '\necho '   Target: 90 days of state versions retained'\necho '   Measure: Age of oldest version'\necho '   Alert: Oldest version < 80 days (lifecycle issue)'\necho ''\necho '=== Monitoring Dashboard (Terraform) ==='\necho ''\necho 'resource \"aws_cloudwatch_dashboard\" \"terraform_dr\" {'\necho '  dashboard_name = \"Terraform-DR-Metrics\"'\necho ''\necho '  dashboard_body = jsonencode({'\necho '    widgets = ['\necho '      {'\necho '        type = \"metric\"'\necho '        properties = {'\necho '          metrics = [[\"AWS/S3\", \"ReplicationLatency\", { stat = \"Maximum\" }]]'\necho '          title   = \"State Replication Lag\"'\necho '          region  = \"us-east-1\"'\necho '        }'\necho '      },'\necho '      {'\necho '        type = \"metric\"'\necho '        properties = {'\necho '          metrics = [[\"TerraformState\", \"StateFileSize\"]]'\necho '          title   = \"State File Size (bytes)\"'\necho '          yAxis = { left = { min = 0 } }'\necho '        }'\necho '      }'\necho '    ]'\necho '  })'\necho '}'\necho ''\necho '=== Sample Alert: State File Corrupted ==='\necho ''\necho 'resource \"aws_cloudwatch_metric_alarm\" \"state_corrupted\" {'\necho '  alarm_name          = \"terraform-state-corrupted\"'\necho '  comparison_operator = \"LessThanThreshold\"'\necho '  evaluation_periods  = 1'\necho '  metric_name         = \"StateFileSize\"'\necho '  namespace           = \"TerraformState\"'\necho '  period              = 60'\necho '  statistic           = \"Average\"'\necho '  threshold           = 100  # Bytes'\necho '  alarm_description   = \"Terraform state file is suspiciously small or empty\"'\necho '  alarm_actions       = [aws_sns_topic.devops_alerts.arn]'\necho '}'",
      "description": "Define and monitor key disaster recovery metrics for Terraform state",
      "explanation": "You can't manage what you don't measure. RTO and RPO are business requirements that guide your DR architecture. Replication lag monitoring ensures your replica is actually synchronized. State file size alerts catch corruption immediately. Backup success rate ensures your safety net is reliable. DR test frequency ensures procedures remain valid as infrastructure evolves. Version retention compliance confirms your lifecycle policies work. Set up CloudWatch dashboards so your team can see DR health at a glance. Alert on anomalies so you detect issues before they become disasters.",
      "what_it_does": "Defines critical DR metrics (RTO, RPO, replication lag) with CloudWatch monitoring and alerting configurations.",
      "next_step": "Cleanup to remove scenario resources.",
      "cleanup": false
    },
    {
      "name": "Step 11: Cleanup",
      "command": "echo '=== Scenario Complete ==='\necho ''\necho 'You have learned:'\necho '✓ How to design disaster recovery ready state backends'\necho '✓ Cross-region replication for state files'\necho '✓ State backup procedures and validation'\necho '✓ Recovering from state corruption using S3 versions'\necho '✓ Regional failover to replica buckets'\necho '✓ Rebuilding state with terraform import'\necho '✓ Automated DR testing scripts'\necho '✓ DR runbook creation and best practices'\necho '✓ Key metrics for DR monitoring'\necho ''\necho '=== Production Checklist ==='\necho ''\necho '□ Enable S3 versioning on state bucket'\necho '□ Configure cross-region replication'\necho '□ Set up DynamoDB state locking in both regions'\necho '□ Enable MFA delete on state bucket'\necho '□ Encrypt state with KMS'\necho '□ Create automated backup scripts'\necho '□ Set up CloudWatch monitoring and alerts'\necho '□ Write and maintain DR runbook'\necho '□ Schedule monthly DR tests'\necho '□ Train team on DR procedures'\necho '□ Document RTO and RPO requirements'\necho '□ Create incident response plan'\necho ''\necho 'In a real environment, you would now:'\necho '  terraform destroy  # Remove all DR infrastructure'\necho ''\necho 'This would remove:'\necho '  - State buckets (primary and replica)'\necho '  - DynamoDB lock tables'\necho '  - Replication IAM roles'\necho '  - CloudWatch alarms and dashboards'\necho ''\necho 'Next: Apply these DR patterns to your production infrastructure!'",
      "description": "Complete the disaster recovery scenario and review what was learned",
      "explanation": "Disaster recovery for Terraform state is critical but often overlooked. The techniques learned here -- S3 versioning, cross-region replication, state locking, backup validation, failover procedures, and import workflows -- form a complete DR strategy. In production, implement all these layers. State corruption is recoverable if you have versioning. Regional failures are survivable if you have replication. Complete state loss is recoverable (with effort) if you have imports. But the best disaster is the one you prevent through good architecture and regular testing.",
      "what_it_does": "Summarizes all disaster recovery concepts learned and provides a production checklist for implementing DR for Terraform.",
      "next_step": "You've completed all Terraform scenarios! Practice these patterns in your own infrastructure.",
      "cleanup": true
    }
  ]
}
