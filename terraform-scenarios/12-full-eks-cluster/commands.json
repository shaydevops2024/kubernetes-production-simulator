{
  "scenario_id": "12-full-eks-cluster",
  "difficulty": "hard",
  "duration": "45 min",
  "commands": [
    {
      "name": "Step 1: Create EKS Cluster Resource with IAM Role",
      "command": "echo '=== EKS Cluster - IAM Role and Cluster Resource ==='\necho ''\necho 'An EKS cluster needs an IAM role that allows the EKS service'\necho 'to manage AWS resources on your behalf.'\necho ''\ncat << 'TFEOF'\n# providers.tf\nterraform {\n  required_version = \">= 1.5.0\"\n\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n    tls = {\n      source  = \"hashicorp/tls\"\n      version = \"~> 4.0\"\n    }\n  }\n\n  backend \"s3\" {\n    bucket         = \"mycompany-terraform-state\"\n    key            = \"eks/production/terraform.tfstate\"\n    region         = \"us-east-1\"\n    dynamodb_table = \"terraform-locks\"\n    encrypt        = true\n  }\n}\n\n# variables.tf\nvariable \"cluster_name\" {\n  description = \"Name of the EKS cluster\"\n  type        = string\n  default     = \"production-eks\"\n}\n\nvariable \"cluster_version\" {\n  description = \"Kubernetes version for the EKS cluster\"\n  type        = string\n  default     = \"1.29\"\n}\n\n# iam.tf - EKS Cluster IAM Role\nresource \"aws_iam_role\" \"eks_cluster\" {\n  name = \"${var.cluster_name}-cluster-role\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [{\n      Action = \"sts:AssumeRole\"\n      Effect = \"Allow\"\n      Principal = {\n        Service = \"eks.amazonaws.com\"\n      }\n    }]\n  })\n\n  tags = {\n    Name = \"${var.cluster_name}-cluster-role\"\n  }\n}\n\nresource \"aws_iam_role_policy_attachment\" \"eks_cluster_policy\" {\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonEKSClusterPolicy\"\n  role       = aws_iam_role.eks_cluster.name\n}\n\nresource \"aws_iam_role_policy_attachment\" \"eks_vpc_resource_controller\" {\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonEKSVPCResourceController\"\n  role       = aws_iam_role.eks_cluster.name\n}\n\n# eks.tf - EKS Cluster\nresource \"aws_eks_cluster\" \"main\" {\n  name     = var.cluster_name\n  version  = var.cluster_version\n  role_arn = aws_iam_role.eks_cluster.arn\n\n  vpc_config {\n    subnet_ids              = concat(\n      aws_subnet.private[*].id,\n      aws_subnet.public[*].id\n    )\n    endpoint_private_access = true\n    endpoint_public_access  = true\n    public_access_cidrs     = [\"0.0.0.0/0\"]  # Restrict in production!\n    security_group_ids      = [aws_security_group.eks_cluster.id]\n  }\n\n  enabled_cluster_log_types = [\n    \"api\", \"audit\", \"authenticator\",\n    \"controllerManager\", \"scheduler\"\n  ]\n\n  depends_on = [\n    aws_iam_role_policy_attachment.eks_cluster_policy,\n    aws_iam_role_policy_attachment.eks_vpc_resource_controller\n  ]\n\n  tags = {\n    Environment = \"production\"\n  }\n}\nTFEOF\necho ''\necho '=== Key Decisions ==='\necho '  - endpoint_private_access=true: Nodes communicate via private network'\necho '  - endpoint_public_access=true: kubectl works from outside the VPC'\necho '  - enabled_cluster_log_types: All 5 log types for full observability'\necho '  - depends_on: Ensures IAM role is ready before cluster creation'\necho '  - Cluster creation takes ~10-15 minutes'",
      "description": "Create the EKS cluster IAM role and cluster resource with logging and network configuration",
      "explanation": "Every EKS cluster needs an IAM role that the EKS service assumes to manage resources like ENIs and security groups. The AmazonEKSClusterPolicy gives EKS permission to manage Kubernetes resources, while AmazonEKSVPCResourceController enables pod networking features. The cluster itself is configured with both private and public API endpoints -- private for node communication and public for developer kubectl access. All five control plane log types are enabled for comprehensive audit trails.",
      "what_it_does": "Displays the complete EKS cluster configuration including provider setup, variables, IAM role with policies, and the cluster resource with VPC configuration and logging.",
      "next_step": "Next we will configure the VPC networking that the cluster depends on.",
      "cleanup": false
    },
    {
      "name": "Step 2: Configure Cluster VPC and Networking",
      "command": "echo '=== EKS VPC Networking ==='\necho ''\necho 'EKS requires a VPC with both public and private subnets across'\necho 'multiple Availability Zones for high availability.'\necho ''\ncat << 'TFEOF'\n# vpc.tf - VPC and Networking for EKS\n\nlocals {\n  azs             = [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"]\n  private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"]\n  public_subnets  = [\"10.0.101.0/24\", \"10.0.102.0/24\", \"10.0.103.0/24\"]\n}\n\nresource \"aws_vpc\" \"eks\" {\n  cidr_block           = \"10.0.0.0/16\"\n  enable_dns_hostnames = true\n  enable_dns_support   = true\n\n  tags = {\n    Name = \"${var.cluster_name}-vpc\"\n    \"kubernetes.io/cluster/${var.cluster_name}\" = \"shared\"\n  }\n}\n\n# Private subnets (for worker nodes)\nresource \"aws_subnet\" \"private\" {\n  count             = length(local.azs)\n  vpc_id            = aws_vpc.eks.id\n  cidr_block        = local.private_subnets[count.index]\n  availability_zone = local.azs[count.index]\n\n  tags = {\n    Name = \"${var.cluster_name}-private-${local.azs[count.index]}\"\n    \"kubernetes.io/cluster/${var.cluster_name}\" = \"shared\"\n    \"kubernetes.io/role/internal-elb\"           = \"1\"\n  }\n}\n\n# Public subnets (for load balancers)\nresource \"aws_subnet\" \"public\" {\n  count                   = length(local.azs)\n  vpc_id                  = aws_vpc.eks.id\n  cidr_block              = local.public_subnets[count.index]\n  availability_zone       = local.azs[count.index]\n  map_public_ip_on_launch = true\n\n  tags = {\n    Name = \"${var.cluster_name}-public-${local.azs[count.index]}\"\n    \"kubernetes.io/cluster/${var.cluster_name}\" = \"shared\"\n    \"kubernetes.io/role/elb\"                    = \"1\"\n  }\n}\n\n# Internet Gateway\nresource \"aws_internet_gateway\" \"eks\" {\n  vpc_id = aws_vpc.eks.id\n  tags   = { Name = \"${var.cluster_name}-igw\" }\n}\n\n# NAT Gateway (one per AZ for HA, or one shared to save cost)\nresource \"aws_eip\" \"nat\" {\n  domain = \"vpc\"\n  tags   = { Name = \"${var.cluster_name}-nat-eip\" }\n}\n\nresource \"aws_nat_gateway\" \"eks\" {\n  allocation_id = aws_eip.nat.id\n  subnet_id     = aws_subnet.public[0].id\n  tags          = { Name = \"${var.cluster_name}-nat\" }\n\n  depends_on = [aws_internet_gateway.eks]\n}\n\n# Route tables\nresource \"aws_route_table\" \"private\" {\n  vpc_id = aws_vpc.eks.id\n  route {\n    cidr_block     = \"0.0.0.0/0\"\n    nat_gateway_id = aws_nat_gateway.eks.id\n  }\n  tags = { Name = \"${var.cluster_name}-private-rt\" }\n}\n\nresource \"aws_route_table\" \"public\" {\n  vpc_id = aws_vpc.eks.id\n  route {\n    cidr_block = \"0.0.0.0/0\"\n    gateway_id = aws_internet_gateway.eks.id\n  }\n  tags = { Name = \"${var.cluster_name}-public-rt\" }\n}\nTFEOF\necho ''\necho '=== Critical EKS Subnet Tags ==='\necho '  kubernetes.io/cluster/<name> = shared'\necho '    Tells EKS this subnet belongs to the cluster'\necho ''\necho '  kubernetes.io/role/internal-elb = 1 (private subnets)'\necho '    AWS Load Balancer Controller places internal ALBs here'\necho ''\necho '  kubernetes.io/role/elb = 1 (public subnets)'\necho '    AWS Load Balancer Controller places public ALBs here'\necho ''\necho '  NAT Gateway: Allows private nodes to pull container images'\necho '  Cost tip: Use 1 NAT GW in dev ($32/mo), 3 NAT GWs in prod (HA)'",
      "description": "Configure the VPC with public/private subnets, NAT gateway, and EKS-required subnet tags",
      "explanation": "EKS requires specific VPC networking. Worker nodes run in private subnets (no public IPs) for security, while load balancers are created in public subnets. The special kubernetes.io tags on subnets tell the AWS Load Balancer Controller where to place ALBs and NLBs. A NAT gateway in the public subnet allows private nodes to reach the internet for pulling container images. For production, you should deploy one NAT gateway per AZ for high availability, though a single shared one saves cost in development.",
      "what_it_does": "Displays the complete VPC configuration with 3 private subnets, 3 public subnets, internet gateway, NAT gateway, route tables, and the critical EKS subnet tags for load balancer discovery.",
      "next_step": "Next we will create the managed node group for worker nodes.",
      "cleanup": false
    },
    {
      "name": "Step 3: Create Managed Node Group with Instance Types",
      "command": "echo '=== EKS Managed Node Group ==='\necho ''\necho 'Managed node groups let AWS handle node provisioning, updates,'\necho 'and lifecycle management for your worker nodes.'\necho ''\ncat << 'TFEOF'\n# node-group.tf - EKS Managed Node Group\n\n# IAM Role for Worker Nodes\nresource \"aws_iam_role\" \"eks_nodes\" {\n  name = \"${var.cluster_name}-node-role\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [{\n      Action = \"sts:AssumeRole\"\n      Effect = \"Allow\"\n      Principal = {\n        Service = \"ec2.amazonaws.com\"\n      }\n    }]\n  })\n}\n\n# Required policies for worker nodes\nresource \"aws_iam_role_policy_attachment\" \"node_AmazonEKSWorkerNodePolicy\" {\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy\"\n  role       = aws_iam_role.eks_nodes.name\n}\n\nresource \"aws_iam_role_policy_attachment\" \"node_AmazonEKS_CNI_Policy\" {\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy\"\n  role       = aws_iam_role.eks_nodes.name\n}\n\nresource \"aws_iam_role_policy_attachment\" \"node_AmazonEC2ContainerRegistryReadOnly\" {\n  policy_arn = \"arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly\"\n  role       = aws_iam_role.eks_nodes.name\n}\n\n# Managed Node Group\nresource \"aws_eks_node_group\" \"general\" {\n  cluster_name    = aws_eks_cluster.main.name\n  node_group_name = \"general-purpose\"\n  node_role_arn   = aws_iam_role.eks_nodes.arn\n  subnet_ids      = aws_subnet.private[*].id\n\n  instance_types = [\"t3.large\", \"t3a.large\"]  # Multiple types for availability\n  capacity_type  = \"ON_DEMAND\"                 # or \"SPOT\" for cost savings\n  disk_size      = 50                          # GB per node\n\n  scaling_config {\n    desired_size = 3\n    min_size     = 2\n    max_size     = 10\n  }\n\n  update_config {\n    max_unavailable = 1   # Rolling updates, one node at a time\n  }\n\n  labels = {\n    role        = \"general\"\n    environment = \"production\"\n  }\n\n  # Optional: Taint nodes for dedicated workloads\n  # taint {\n  #   key    = \"dedicated\"\n  #   value  = \"gpu\"\n  #   effect = \"NO_SCHEDULE\"\n  # }\n\n  depends_on = [\n    aws_iam_role_policy_attachment.node_AmazonEKSWorkerNodePolicy,\n    aws_iam_role_policy_attachment.node_AmazonEKS_CNI_Policy,\n    aws_iam_role_policy_attachment.node_AmazonEC2ContainerRegistryReadOnly\n  ]\n\n  lifecycle {\n    ignore_changes = [scaling_config[0].desired_size]\n  }\n\n  tags = {\n    Name = \"${var.cluster_name}-general-nodes\"\n  }\n}\nTFEOF\necho ''\necho '=== Key Design Decisions ==='\necho '  - Multiple instance_types: Increases availability across AZs'\necho '  - capacity_type ON_DEMAND: Reliable for production (SPOT saves 60-90%)'\necho '  - scaling: 2-10 nodes with Cluster Autoscaler managing desired_size'\necho '  - ignore_changes on desired_size: Prevents Terraform from overriding'\necho '    what the Cluster Autoscaler sets'\necho '  - max_unavailable=1: Safe rolling updates without downtime'\necho '  - Nodes in PRIVATE subnets: No public IP exposure'",
      "description": "Create a managed node group with auto-scaling, multiple instance types, and proper IAM policies",
      "explanation": "Managed node groups abstract away the EC2 Auto Scaling Group complexity. Worker nodes need their own IAM role with three policies: EKSWorkerNodePolicy (node registration), EKS_CNI_Policy (pod networking), and EC2ContainerRegistryReadOnly (pulling images from ECR). Listing multiple instance types improves availability when one type is exhausted in an AZ. The ignore_changes lifecycle rule on desired_size is critical -- it prevents Terraform from fighting with the Cluster Autoscaler, which dynamically adjusts the count.",
      "what_it_does": "Displays the managed node group configuration with worker node IAM role, three required policies, auto-scaling (2-10 nodes), multiple instance types, and lifecycle rules for Cluster Autoscaler compatibility.",
      "next_step": "Next we will set up the OIDC provider for IAM Roles for Service Accounts (IRSA).",
      "cleanup": false
    },
    {
      "name": "Step 4: Set Up OIDC Provider for IRSA",
      "command": "echo '=== OIDC Provider for IAM Roles for Service Accounts (IRSA) ==='\necho ''\necho 'IRSA allows Kubernetes pods to assume IAM roles directly,'\necho 'without using node-level permissions or kube2iam hacks.'\necho ''\necho '=== How IRSA Works ==='\necho '  1. EKS cluster exposes an OIDC issuer URL'\necho '  2. You create an IAM OIDC provider that trusts that URL'\necho '  3. IAM roles have trust policies referencing the OIDC provider'\necho '  4. Kubernetes Service Accounts are annotated with the IAM role ARN'\necho '  5. AWS SDK in the pod uses the projected service account token'\necho '     to assume the IAM role via STS'\necho ''\ncat << 'TFEOF'\n# oidc.tf - OIDC Provider for IRSA\n\n# Extract the OIDC issuer URL from the EKS cluster\ndata \"tls_certificate\" \"eks\" {\n  url = aws_eks_cluster.main.identity[0].oidc[0].issuer\n}\n\n# Create the IAM OIDC provider\nresource \"aws_iam_openid_connect_provider\" \"eks\" {\n  client_id_list  = [\"sts.amazonaws.com\"]\n  thumbprint_list = [data.tls_certificate.eks.certificates[0].sha1_fingerprint]\n  url             = aws_eks_cluster.main.identity[0].oidc[0].issuer\n\n  tags = {\n    Name = \"${var.cluster_name}-oidc\"\n  }\n}\n\n# Helper local for constructing trust policies\nlocals {\n  oidc_provider_arn = aws_iam_openid_connect_provider.eks.arn\n  oidc_provider_url = replace(\n    aws_iam_openid_connect_provider.eks.url, \"https://\", \"\"\n  )\n}\nTFEOF\necho ''\necho '=== OIDC Flow Diagram ==='\necho ''\necho '  Pod (with ServiceAccount) -> STS AssumeRoleWithWebIdentity'\necho '      |                              |'\necho '      | projected SA token            | validates token against'\necho '      |                              | OIDC provider'\necho '      v                              v'\necho '  EKS OIDC Issuer  <------>  IAM OIDC Provider'\necho '                                     |'\necho '                                     v'\necho '                              IAM Role (with trust policy)'\necho '                                     |'\necho '                                     v'\necho '                              Temporary AWS credentials'\necho '                              returned to the pod'\necho ''\necho '=== Why IRSA is critical ==='\necho '  Before IRSA: All pods on a node shared the node IAM role'\necho '    - Any pod could access S3, DynamoDB, etc.'\necho '    - No way to give Pod A S3 access but deny Pod B'\necho ''\necho '  With IRSA: Each pod gets its own IAM role'\necho '    - Fine-grained, per-pod permissions'\necho '    - No credential files or environment variables needed'\necho '    - Automatic credential rotation'",
      "description": "Set up the OIDC identity provider that enables IAM Roles for Service Accounts (IRSA)",
      "explanation": "IRSA is one of the most important EKS security features. Before IRSA, all pods on a node shared the node's IAM role, which violated the principle of least privilege. With IRSA, each Kubernetes ServiceAccount can be linked to a specific IAM role. The EKS cluster has a built-in OIDC issuer, and you create an IAM OIDC provider that trusts it. When a pod with an annotated ServiceAccount makes AWS API calls, the AWS SDK uses STS AssumeRoleWithWebIdentity to get temporary credentials scoped to that specific role.",
      "what_it_does": "Creates the IAM OIDC identity provider from the EKS cluster's OIDC issuer, explains the IRSA authentication flow, and shows why per-pod IAM roles are critical for security.",
      "next_step": "Next we will configure an IAM role for a specific service account.",
      "cleanup": false
    },
    {
      "name": "Step 5: Configure IAM Role for a Service Account",
      "command": "echo '=== IRSA: Creating an IAM Role for a Service Account ==='\necho ''\necho 'Example: Give the \"app-backend\" service account access to an S3 bucket'\necho 'and a DynamoDB table, but nothing else.'\necho ''\ncat << 'TFEOF'\n# irsa-app-backend.tf\n\n# IAM Role that the service account will assume\nresource \"aws_iam_role\" \"app_backend\" {\n  name = \"${var.cluster_name}-app-backend\"\n\n  assume_role_policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [{\n      Effect = \"Allow\"\n      Principal = {\n        Federated = local.oidc_provider_arn\n      }\n      Action = \"sts:AssumeRoleWithWebIdentity\"\n      Condition = {\n        StringEquals = {\n          \"${local.oidc_provider_url}:aud\" = \"sts.amazonaws.com\"\n          \"${local.oidc_provider_url}:sub\" = \"system:serviceaccount:app:app-backend\"\n        }\n      }\n    }]\n  })\n}\n\n# Policy: S3 access for the app\nresource \"aws_iam_role_policy\" \"app_backend_s3\" {\n  name = \"s3-access\"\n  role = aws_iam_role.app_backend.id\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [{\n      Effect = \"Allow\"\n      Action = [\n        \"s3:GetObject\",\n        \"s3:PutObject\",\n        \"s3:ListBucket\"\n      ]\n      Resource = [\n        \"arn:aws:s3:::myapp-data-prod\",\n        \"arn:aws:s3:::myapp-data-prod/*\"\n      ]\n    }]\n  })\n}\n\n# Policy: DynamoDB access for the app\nresource \"aws_iam_role_policy\" \"app_backend_dynamodb\" {\n  name = \"dynamodb-access\"\n  role = aws_iam_role.app_backend.id\n\n  policy = jsonencode({\n    Version = \"2012-10-17\"\n    Statement = [{\n      Effect = \"Allow\"\n      Action = [\n        \"dynamodb:GetItem\",\n        \"dynamodb:PutItem\",\n        \"dynamodb:Query\"\n      ]\n      Resource = \"arn:aws:dynamodb:us-east-1:*:table/myapp-sessions\"\n    }]\n  })\n}\n\n# Output the role ARN for the Kubernetes manifest\noutput \"app_backend_role_arn\" {\n  value = aws_iam_role.app_backend.arn\n}\nTFEOF\necho ''\necho '=== Kubernetes Side (ServiceAccount manifest) ==='\necho ''\ncat << 'K8SEOF'\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: app-backend\n  namespace: app\n  annotations:\n    # This annotation links the SA to the IAM role\n    eks.amazonaws.com/role-arn: arn:aws:iam::123456789012:role/production-eks-app-backend\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-backend\n  namespace: app\nspec:\n  template:\n    spec:\n      serviceAccountName: app-backend  # Uses the annotated SA\n      containers:\n        - name: app\n          image: myapp:latest\n          # AWS SDK auto-detects IRSA credentials\n          # No AWS_ACCESS_KEY_ID needed!\nK8SEOF\necho ''\necho '=== Security: Trust Policy Conditions ==='\necho '  The :sub condition locks the role to a SPECIFIC service account:'\necho '    system:serviceaccount:<namespace>:<sa-name>'\necho '  Without this, ANY service account in the cluster could assume the role!'",
      "description": "Create an IAM role with fine-grained permissions scoped to a specific Kubernetes service account",
      "explanation": "This is where IRSA comes together. The IAM role trust policy uses two critical conditions: the :aud condition ensures only STS can use the token, and the :sub condition restricts the role to a specific namespace and service account (system:serviceaccount:app:app-backend). Without the :sub condition, any pod in the cluster could assume the role. On the Kubernetes side, the ServiceAccount is annotated with the IAM role ARN. The AWS SDK automatically detects the projected token and uses AssumeRoleWithWebIdentity.",
      "what_it_does": "Creates a complete IRSA setup: IAM role with a trust policy locked to a specific service account, fine-grained S3 and DynamoDB policies, and the corresponding Kubernetes ServiceAccount and Deployment manifests.",
      "next_step": "Next we will install essential EKS cluster add-ons.",
      "cleanup": false
    },
    {
      "name": "Step 6: Install Cluster Add-ons",
      "command": "echo '=== EKS Cluster Add-ons ==='\necho ''\necho 'Add-ons are operational software that EKS needs to function.'\necho 'Managing them via Terraform ensures version consistency.'\necho ''\ncat << 'TFEOF'\n# addons.tf - EKS Managed Add-ons\n\n# VPC CNI - Pod networking (assigns VPC IPs to pods)\nresource \"aws_eks_addon\" \"vpc_cni\" {\n  cluster_name = aws_eks_cluster.main.name\n  addon_name   = \"vpc-cni\"\n\n  # Use latest compatible version\n  addon_version            = \"v1.16.0-eksbuild.1\"\n  resolve_conflicts_on_update = \"OVERWRITE\"\n\n  # Use IRSA for the CNI plugin (best practice)\n  service_account_role_arn = aws_iam_role.vpc_cni.arn\n\n  configuration_values = jsonencode({\n    env = {\n      # Enable prefix delegation for higher pod density\n      ENABLE_PREFIX_DELEGATION = \"true\"\n      WARM_PREFIX_TARGET       = \"1\"\n    }\n  })\n}\n\n# CoreDNS - Cluster DNS resolution\nresource \"aws_eks_addon\" \"coredns\" {\n  cluster_name = aws_eks_cluster.main.name\n  addon_name   = \"coredns\"\n\n  addon_version            = \"v1.11.1-eksbuild.6\"\n  resolve_conflicts_on_update = \"OVERWRITE\"\n\n  configuration_values = jsonencode({\n    replicaCount = 3    # HA: one per AZ\n    resources = {\n      limits = {\n        cpu    = \"100m\"\n        memory = \"150Mi\"\n      }\n      requests = {\n        cpu    = \"100m\"\n        memory = \"150Mi\"\n      }\n    }\n  })\n\n  depends_on = [aws_eks_node_group.general]\n}\n\n# kube-proxy - Network rules for Service routing\nresource \"aws_eks_addon\" \"kube_proxy\" {\n  cluster_name = aws_eks_cluster.main.name\n  addon_name   = \"kube-proxy\"\n\n  addon_version            = \"v1.29.0-eksbuild.3\"\n  resolve_conflicts_on_update = \"OVERWRITE\"\n}\n\n# Optional: EBS CSI Driver (for persistent volumes)\nresource \"aws_eks_addon\" \"ebs_csi\" {\n  cluster_name = aws_eks_cluster.main.name\n  addon_name   = \"aws-ebs-csi-driver\"\n\n  addon_version            = \"v1.28.0-eksbuild.1\"\n  resolve_conflicts_on_update = \"OVERWRITE\"\n  service_account_role_arn = aws_iam_role.ebs_csi.arn\n}\nTFEOF\necho ''\necho '=== What Each Add-on Does ==='\necho '  vpc-cni:    Assigns VPC IP addresses to pods (AWS native networking)'\necho '              Prefix delegation: 110+ pods per node (vs 29 default)'\necho '  coredns:    DNS resolution inside the cluster (service discovery)'\necho '  kube-proxy: Maintains iptables/IPVS rules for Kubernetes Services'\necho '  ebs-csi:    Enables EBS-backed PersistentVolumes (databases, etc.)'\necho ''\necho '=== Version Management ==='\necho '  Check available versions:'\necho '  aws eks describe-addon-versions --addon-name vpc-cni --kubernetes-version 1.29'",
      "description": "Install and configure essential EKS add-ons: vpc-cni, coredns, kube-proxy, and ebs-csi-driver",
      "explanation": "EKS add-ons are critical components that run inside the cluster. The VPC CNI plugin is unique to EKS -- it assigns real VPC IP addresses to pods, enabling native VPC networking without overlays. Enabling prefix delegation dramatically increases pod density (110+ per node instead of 29). CoreDNS handles cluster-internal DNS, so services can find each other by name. kube-proxy maintains the network rules that make Kubernetes Services work. The EBS CSI driver enables persistent storage for stateful workloads like databases.",
      "what_it_does": "Configures four EKS managed add-ons with specific versions, IRSA roles where applicable, custom configuration (prefix delegation for VPC CNI, 3 replicas for CoreDNS), and explains what each add-on does.",
      "next_step": "Next we will configure kubeconfig for kubectl access to the cluster.",
      "cleanup": false
    },
    {
      "name": "Step 7: Configure Kubectl and Kubeconfig Access",
      "command": "echo '=== Kubeconfig Generation for EKS ==='\necho ''\necho 'After the cluster is created, you need to configure kubectl to access it.'\necho ''\ncat << 'TFEOF'\n# outputs.tf - Cluster Access Configuration\n\noutput \"cluster_endpoint\" {\n  description = \"EKS cluster API endpoint\"\n  value       = aws_eks_cluster.main.endpoint\n}\n\noutput \"cluster_certificate_authority\" {\n  description = \"Base64 encoded certificate data for the cluster\"\n  value       = aws_eks_cluster.main.certificate_authority[0].data\n  sensitive   = true\n}\n\noutput \"cluster_name\" {\n  description = \"EKS cluster name\"\n  value       = aws_eks_cluster.main.name\n}\n\noutput \"cluster_oidc_issuer_url\" {\n  description = \"OIDC issuer URL for IRSA\"\n  value       = aws_eks_cluster.main.identity[0].oidc[0].issuer\n}\n\noutput \"configure_kubectl\" {\n  description = \"Command to configure kubectl\"\n  value       = \"aws eks update-kubeconfig --region us-east-1 --name ${aws_eks_cluster.main.name}\"\n}\nTFEOF\necho ''\necho '=== Accessing the Cluster ==='\necho ''\necho '  # After terraform apply, configure kubectl:'\necho '  $ aws eks update-kubeconfig --region us-east-1 --name production-eks'\necho ''\necho '  Added new context arn:aws:eks:us-east-1:123456789012:cluster/production-eks'\necho '  to /home/user/.kube/config'\necho ''\necho '  # Verify access:'\necho '  $ kubectl get nodes'\necho '  NAME                             STATUS   ROLES    AGE   VERSION'\necho '  ip-10-0-1-45.ec2.internal        Ready    <none>   5m    v1.29.0-eks-abcdef'\necho '  ip-10-0-2-78.ec2.internal        Ready    <none>   5m    v1.29.0-eks-abcdef'\necho '  ip-10-0-3-12.ec2.internal        Ready    <none>   5m    v1.29.0-eks-abcdef'\necho ''\necho '  $ kubectl get pods -n kube-system'\necho '  NAME                       READY   STATUS    RESTARTS   AGE'\necho '  aws-node-xxxxx             2/2     Running   0          5m'\necho '  coredns-yyyyy-aaaaa        1/1     Running   0          5m'\necho '  coredns-yyyyy-bbbbb        1/1     Running   0          5m'\necho '  coredns-yyyyy-ccccc        1/1     Running   0          5m'\necho '  kube-proxy-zzzzz           1/1     Running   0          5m'\necho ''\necho '=== Access Control ==='\necho '  EKS uses aws-auth ConfigMap to map IAM entities to K8s RBAC:'\necho '  - Node IAM role is auto-mapped (nodes can register)'\necho '  - Add IAM users/roles for developer access'\necho '  - Use EKS Access Entries (v1.28+) for simpler IAM-to-K8s mapping'",
      "description": "Configure kubectl access to the EKS cluster with outputs and kubeconfig generation",
      "explanation": "After Terraform creates the cluster, you need to configure kubectl to communicate with it. The aws eks update-kubeconfig command generates a kubeconfig that uses the AWS CLI for authentication -- when you run kubectl commands, it calls aws eks get-token behind the scenes to get a short-lived token. The cluster outputs (endpoint, CA data, name) are also useful for CI/CD systems that need cluster access. EKS Access Entries (Kubernetes 1.28+) simplify the IAM-to-Kubernetes RBAC mapping that previously required managing the aws-auth ConfigMap.",
      "what_it_does": "Shows Terraform outputs for cluster access, the aws eks update-kubeconfig command, simulated kubectl verification output showing healthy nodes and system pods, and explains EKS access control.",
      "next_step": "Next we will verify the complete EKS setup.",
      "cleanup": false
    },
    {
      "name": "Step 8: Verify Complete EKS Setup",
      "command": "echo '=== Complete EKS Cluster Verification ==='\necho ''\necho '  $ terraform output'\necho '  cluster_endpoint             = \"https://ABCD1234.gr7.us-east-1.eks.amazonaws.com\"'\necho '  cluster_name                 = \"production-eks\"'\necho '  cluster_oidc_issuer_url      = \"https://oidc.eks.us-east-1.amazonaws.com/id/ABCD1234\"'\necho '  configure_kubectl            = \"aws eks update-kubeconfig --region us-east-1 --name production-eks\"'\necho ''\necho '=== Resource Summary ==='\necho ''\necho '  $ terraform state list | wc -l'\necho '  28'\necho ''\necho '  Networking (10 resources):'\necho '    aws_vpc.eks'\necho '    aws_subnet.private[0,1,2]'\necho '    aws_subnet.public[0,1,2]'\necho '    aws_internet_gateway.eks'\necho '    aws_nat_gateway.eks + aws_eip.nat'\necho '    aws_route_table.private + aws_route_table.public'\necho ''\necho '  IAM (8 resources):'\necho '    aws_iam_role.eks_cluster + 2 policy attachments'\necho '    aws_iam_role.eks_nodes + 3 policy attachments'\necho '    aws_iam_role.app_backend + policies'\necho ''\necho '  EKS (6 resources):'\necho '    aws_eks_cluster.main'\necho '    aws_eks_node_group.general'\necho '    aws_iam_openid_connect_provider.eks'\necho '    aws_eks_addon.vpc_cni'\necho '    aws_eks_addon.coredns'\necho '    aws_eks_addon.kube_proxy'\necho ''\necho '=== Production Checklist ==='\necho '  [x] VPC with private/public subnets across 3 AZs'\necho '  [x] NAT Gateway for private node internet access'\necho '  [x] EKS cluster with all 5 control plane log types'\necho '  [x] Managed node group with auto-scaling (2-10 nodes)'\necho '  [x] OIDC provider for IRSA (per-pod IAM roles)'\necho '  [x] Sample IRSA role with least-privilege policies'\necho '  [x] Core add-ons: vpc-cni, coredns, kube-proxy'\necho '  [x] Kubeconfig access configured'\necho ''\necho '=== What to add next ==='\necho '  [ ] Cluster Autoscaler or Karpenter for node scaling'\necho '  [ ] AWS Load Balancer Controller for ALB/NLB ingress'\necho '  [ ] External DNS for automatic Route53 records'\necho '  [ ] Secrets Store CSI Driver for AWS Secrets Manager'\necho '  [ ] Monitoring: Prometheus + Grafana or CloudWatch Container Insights'",
      "description": "Verify the complete EKS setup with resource summary and production checklist",
      "explanation": "A production EKS cluster involves roughly 25-30 Terraform resources working together. This verification step confirms that all the pieces are in place: networking provides the foundation, IAM roles enable secure access, the EKS cluster and node group provide compute, OIDC enables IRSA for pod-level permissions, and add-ons provide essential cluster services. The production checklist helps ensure nothing was missed, while the 'what to add next' section outlines common extensions like autoscaling, ingress, and monitoring.",
      "what_it_does": "Displays terraform output values, a categorized resource summary (28 resources across networking, IAM, and EKS), a production readiness checklist, and recommended next steps for enhancing the cluster.",
      "next_step": "Cleanup step to complete the scenario.",
      "cleanup": false
    },
    {
      "name": "Step 9: Cleanup",
      "command": "echo '=== Scenario Complete: Full EKS Cluster ==='\necho ''\necho 'In a real environment, you would destroy the cluster with:'\necho '  terraform destroy'\necho ''\necho 'IMPORTANT: Destroy order matters for EKS!'\necho '  1. Delete Kubernetes resources first (Services with LoadBalancers)'\necho '     kubectl delete svc --all --all-namespaces'\necho '  2. Then run terraform destroy'\necho '  3. If destroy hangs, check for:'\necho '     - ENIs still attached to the VPC (from ALBs)'\necho '     - Security groups with cross-references'\necho '     - Subnets with resources still running'\necho ''\necho '  Estimated destroy time: ~15-20 minutes'\necho '  Estimated monthly cost of this setup: ~$200-300/month'\necho '    - EKS control plane: $72/month'\necho '    - 3x t3.large nodes: ~$180/month'\necho '    - NAT Gateway: ~$32/month + data transfer'\necho ''\necho 'What you learned:'\necho '  1. EKS cluster creation with IAM roles and logging'\necho '  2. VPC networking with public/private subnets and EKS tags'\necho '  3. Managed node groups with auto-scaling and multiple instance types'\necho '  4. OIDC provider setup for IAM Roles for Service Accounts (IRSA)'\necho '  5. Fine-grained per-pod IAM with IRSA trust policies'\necho '  6. Essential add-ons: vpc-cni, coredns, kube-proxy, ebs-csi'\necho '  7. Kubeconfig generation and cluster access verification'\necho '  8. Production checklist and cost estimation'\necho ''\necho 'This completes the Terraform scenarios!'",
      "description": "Clean up all resources created during this scenario",
      "explanation": "Destroying an EKS cluster requires care because Kubernetes resources like LoadBalancer Services create AWS resources (ALBs, ENIs) outside of Terraform's management. These must be deleted first, or terraform destroy will hang waiting for the VPC to be empty. The estimated monthly cost breakdown helps teams understand the financial impact of running EKS. In production, consider using Spot instances for non-critical workloads to reduce the node cost by 60-90%.",
      "what_it_does": "Provides the proper EKS destroy procedure, explains common pitfalls that cause destroy to hang, estimates monthly costs, and summarizes all concepts learned in this scenario.",
      "next_step": "Congratulations on completing the Terraform scenario track!",
      "cleanup": true
    }
  ]
}
