{
  "scenario_id": "01-hpa-autoscaling",
  "difficulty": "medium",
  "duration": "20 min",
  "commands": [
    {
      "name": "Step 1: Create Scenarios Namespace",
      "command": "kubectl create namespace scenarios --dry-run=client -o yaml | kubectl apply -f -",
      "description": "Create dedicated 'scenarios' namespace for all learning scenarios",
      "explanation": "This creates a separate namespace called 'scenarios' where all scenario resources will be deployed. Using --dry-run with apply makes it idempotent - safe to run multiple times.",
      "what_it_does": "Creates the 'scenarios' namespace if it doesn't exist, or does nothing if it already exists.",
      "next_step": "Namespace ready. Now deploy the HPA demo application."
    },
    {
      "name": "Step 2: Deploy Application with Resources",
      "command": "kubectl apply -f k8s-scenarios/01-hpa-autoscaling/deployment.yaml -f k8s-scenarios/01-hpa-autoscaling/service.yaml -n scenarios",
      "description": "Deploy PHP Apache application with CPU/memory resource requests, and a cluster IP",
      "explanation": "HPA requires resource requests to calculate CPU percentage. This deployment has requests (200m CPU, 128Mi memory) and limits (500m CPU, 256Mi memory). Starts with 2 replicas.",
      "what_it_does": "Creates deployment 'hpa-demo-app' and service 'hpa-demo-service' in scenarios namespace.",
      "next_step": "Wait for pods to be Running before creating HPA."
    },
    {
      "name": "Step 3: Verify Pods Running",
      "command": "kubectl get pods -n scenarios -l app=hpa-demo",
      "description": "Check that application pods are running",
      "explanation": "Before creating HPA, ensure the deployment is healthy. You should see 2 pods in Running state with 1/1 containers ready.",
      "what_it_does": "Lists all pods with label app=hpa-demo in scenarios namespace.",
      "next_step": "Once pods show Running status, create the HPA."
    },
    {
      "name": "Step 4: Create HPA",
      "command": "kubectl apply -f k8s-scenarios/01-hpa-autoscaling/hpa.yaml -n scenarios",
      "description": "Create Horizontal Pod Autoscaler targeting 50% CPU",
      "explanation": "HPA monitors CPU usage every 15 seconds. When average CPU exceeds 50%, it scales up (max 10 pods). When CPU drops below 50%, it scales down (min 2 pods). Scale-down is slower to prevent flapping.",
      "what_it_does": "Creates HPA 'hpa-demo' that will automatically adjust replica count based on CPU utilization.",
      "next_step": "HPA is now monitoring. Check its status to see current metrics."
    },
    {
      "name": "Step 5: Check HPA Status",
      "command": "kubectl get hpa hpa-demo -n scenarios",
      "description": "View HPA current metrics and replica count",
      "explanation": "The TARGETS column shows current/target CPU (e.g., 5%/50%). REPLICAS shows current pod count. Initially CPU will be low (0-10%) since there's no load.",
      "what_it_does": "Displays HPA status with current CPU percentage, target, min/max replicas, and current replica count.",
      "next_step": "CPU is low. Generate load to trigger scaling."
    },
    {
      "name": "Step 6: Generate CPU Load",
      "command": "kubectl run load-generator --rm -i --tty --image=busybox --restart=Never -n scenarios -- /bin/sh -c \"while true; do wget -q -O- http://hpa-demo-service; done\"",
      "description": "‚ö†Ô∏è OPEN A NEW TERMINAL WINDOW - Create load generator pod to increase CPU usage",
      "explanation": "‚ö†Ô∏è Run this in a SEPARATE terminal window! This runs an infinite loop making HTTP requests to the service, increasing CPU load. Run for 2-3 minutes then press Ctrl+C. The HPA will detect rising CPU and scale up pods.",
      "what_it_does": "Creates temporary pod that continuously sends requests, generating CPU load on the application. This command will occupy your terminal, so you need another terminal to watch the scaling happen.",
      "next_step": "Leave running 2-3 minutes. In your MAIN terminal window, proceed to Step 7 to watch the HPA scale up in real-time."
    },
    {
      "name": "Step 7: Watch HPA Scale Up",
      "command": "kubectl get hpa hpa-demo -n scenarios -w",
      "description": "Monitor HPA scaling in real-time (Ctrl+C to stop)",
      "explanation": "Watch mode (-w) shows live updates. You'll see TARGETS increase (e.g., 87%/50%) and REPLICAS grow (2 ‚Üí 4 ‚Üí 6...). Scaling takes 1-3 minutes. New pods are created when CPU consistently exceeds 50%.",
      "what_it_does": "Streams live HPA status showing CPU percentage climbing and replica count increasing.",
      "next_step": "Once you see scaling (4+ pods), press Ctrl+C here and stop the load generator."
    },
    {
      "name": "Step 8: Verify Scaled Pods",
      "command": "kubectl get pods -n scenarios -l app=hpa-demo",
      "description": "Count pods after scale-up",
      "explanation": "After stopping load, check final pod count. You should see more than 2 pods (likely 4-8 depending on load generated). All should be Running.",
      "what_it_does": "Lists all hpa-demo pods showing the scaled-up count.",
      "next_step": "Load stopped. Now watch HPA scale down pods."
    },
    {
      "name": "Step 9: Watch HPA Scale Down",
      "command": "kubectl get hpa hpa-demo -n scenarios -w",
      "description": "Watch HPA scale down after load stops (Ctrl+C after 3-5 minutes)",
      "explanation": "Scale-down is slower than scale-up (by design). Wait 3-5 minutes and you'll see TARGETS drop (e.g., 2%/50%) and REPLICAS decrease back toward 2. This prevents flapping from temporary load spikes.",
      "what_it_does": "Streams live HPA status showing CPU percentage dropping and replicas gradually decreasing.",
      "next_step": "Ready for cleanup!",
      "show_yaml": true
    },
    {
      "name": "Cleanup 1: Delete HPA",
      "command": "kubectl delete hpa hpa-demo -n scenarios",
      "description": "Remove the HorizontalPodAutoscaler",
      "explanation": "Deleting HPA stops automatic scaling. The deployment continues running with its current replica count.",
      "what_it_does": "Removes the HPA resource from scenarios namespace.",
      "next_step": "HPA deleted. Now remove the application.",
      "cleanup": true
    },
    {
      "name": "Cleanup 2: Delete Application",
      "command": "kubectl delete -f k8s-scenarios/01-hpa-autoscaling/deployment.yaml -f k8s-scenarios/01-hpa-autoscaling/service.yaml -n scenarios",
      "description": "Remove deployment and service",
      "explanation": "This deletes the deployment, service, and all pods created during the scenario.",
      "what_it_does": "Removes hpa-demo-app deployment, hpa-demo-service, and all associated pods.",
      "next_step": "Resources deleted. Now review what you learned!",
      "cleanup": true
    },
    {
      "name": "üìö Summary: What You Learned",
      "command": "echo '‚úÖ HPA Autoscaling Scenario Complete!'",
      "description": "Review what you learned and key commands",
      "explanation": "Congratulations! You've completed the HPA Autoscaling scenario. Here's what you learned:\n\nüéØ **Key Concepts Learned:**\n‚Ä¢ How HPA monitors CPU metrics every 15 seconds\n‚Ä¢ Scale-up behavior: Fast (1-3 min) when CPU exceeds target (50%)\n‚Ä¢ Scale-down behavior: Slow (5+ min) to prevent flapping\n‚Ä¢ Min/max replica bounds (2-10 pods)\n‚Ä¢ Why resource requests are CRITICAL for HPA\n‚Ä¢ CPU percentage calculation: actual CPU / requested CPU √ó 100\n\nüìã **Commands You Used:**\n\n1Ô∏è‚É£ kubectl create namespace scenarios\n   ‚Üí Created dedicated namespace for scenarios\n\n2Ô∏è‚É£ kubectl apply -f deployment.yaml -f service.yaml\n   ‚Üí Deployed PHP-Apache app (2 replicas) with CPU requests (200m)\n\n3Ô∏è‚É£ kubectl get pods -n scenarios -l app=hpa-demo\n   ‚Üí Verified pods are running before creating HPA\n\n4Ô∏è‚É£ kubectl apply -f hpa.yaml\n   ‚Üí Created HPA targeting 50% CPU, min 2 / max 10 pods\n\n5Ô∏è‚É£ kubectl get hpa -n scenarios\n   ‚Üí Checked HPA status (TARGETS, REPLICAS, min/max)\n\n6Ô∏è‚É£ kubectl run load-generator (in separate terminal)\n   ‚Üí Generated CPU load to trigger scaling\n\n7Ô∏è‚É£ kubectl get hpa -n scenarios -w\n   ‚Üí Watched real-time scaling (CPU climb, replicas increase)\n\n8Ô∏è‚É£ kubectl get pods -n scenarios -l app=hpa-demo\n   ‚Üí Verified scaled-up pod count (4-8 pods)\n\n9Ô∏è‚É£ kubectl get hpa -n scenarios -w (after stopping load)\n   ‚Üí Watched scale-down (CPU drop, replicas decrease)\n\nüîü kubectl delete hpa / deployment / service\n   ‚Üí Cleaned up all resources\n\nüí° **Pro Tips:**\n‚Ä¢ HPA requires resources.requests.cpu - without it, HPA shows <unknown>\n‚Ä¢ Target 50-70% utilization for best balance\n‚Ä¢ Scale-up is fast, scale-down is slow (prevents flapping)\n‚Ä¢ Use 'kubectl top pods' to see actual CPU/memory usage\n‚Ä¢ For production, use custom metrics (req/sec) not just CPU\n\nüöÄ **Next Steps:**\n‚Ä¢ Try changing target utilization in hpa.yaml (30% or 80%)\n‚Ä¢ Experiment with different min/max replicas\n‚Ä¢ Monitor with: kubectl describe hpa hpa-demo -n scenarios\n‚Ä¢ Check the 'Explain YAML Files' button for deep dive into configuration!",
      "what_it_does": "Displays a comprehensive summary of concepts learned and all commands used with explanations.",
      "next_step": "Great job! You now understand Horizontal Pod Autoscaling. Try other scenarios or click 'Explain YAML Files' to learn about the configuration details.",
    }
  ]
}