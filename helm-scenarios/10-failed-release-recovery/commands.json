{
  "scenario_id": "10-failed-release-recovery",
  "difficulty": "hard",
  "duration": "20 min",
  "commands": [
    {
      "name": "Step 1: Create Namespace",
      "command": "kubectl create namespace helm-scenarios --dry-run=client -o yaml | kubectl apply -f -",
      "description": "Create the helm-scenarios namespace",
      "explanation": "Creates the helm-scenarios namespace if it does not already exist. This namespace is shared across all Helm scenarios.",
      "what_it_does": "Creates 'helm-scenarios' namespace if needed.",
      "next_step": "Install the chart with working values to establish a healthy baseline.",
      "cleanup": false
    },
    {
      "name": "Step 2: Install Working Release",
      "command": "helm install recovery-demo helm-scenarios/10-failed-release-recovery/ -f helm-scenarios/10-failed-release-recovery/values.yaml -n helm-scenarios --wait --timeout 90s",
      "description": "Install the recovery-demo chart with valid values (nginx:1.24-alpine, 2 replicas)",
      "explanation": "Installs the custom chart with known-good values. This creates a deployment with 2 nginx:1.24-alpine pods and a ClusterIP service. The --wait flag ensures the deployment is fully healthy before continuing. This establishes our baseline 'good state' at revision 1.",
      "what_it_does": "Deploys recovery-demo with working configuration (revision 1).",
      "next_step": "Verify the release is healthy before breaking it.",
      "cleanup": false
    },
    {
      "name": "Step 3: Verify Healthy Release",
      "command": "helm status recovery-demo -n helm-scenarios && echo '---' && kubectl get pods -n helm-scenarios -l app.kubernetes.io/instance=recovery-demo",
      "description": "Confirm the release is in 'deployed' state with healthy pods",
      "explanation": "The status should show 'STATUS: deployed' and all pods should be Running with 1/1 containers ready. This is our known-good state that we can later roll back to.",
      "what_it_does": "Displays release status and pod health for the baseline deployment.",
      "next_step": "Simulate a failed upgrade using a non-existent image.",
      "cleanup": false
    },
    {
      "name": "Step 4: Simulate Failed Upgrade",
      "command": "helm upgrade recovery-demo helm-scenarios/10-failed-release-recovery/ -f helm-scenarios/10-failed-release-recovery/bad-values.yaml -n helm-scenarios --wait --timeout 30s 2>&1 || true",
      "description": "Attempt an upgrade with bad values (non-existent image tag) -- this will fail",
      "explanation": "This upgrade uses bad-values.yaml which specifies the non-existent image 'nginx:99.99.99-nonexistent'. The pods will enter ImagePullBackOff and never become ready. With --timeout 30s, Helm will wait 30 seconds for pods to become ready, then mark the release as FAILED. The '|| true' prevents the error from stopping the scenario.",
      "what_it_does": "Attempts an upgrade that is designed to fail (creates revision 2 in FAILED state).",
      "next_step": "Check the release status to see the FAILED state.",
      "cleanup": false
    },
    {
      "name": "Step 5: Check Failed Release Status",
      "command": "helm status recovery-demo -n helm-scenarios",
      "description": "View the release status showing FAILED state",
      "explanation": "The status now shows 'STATUS: failed'. This means the last operation (upgrade to revision 2) did not complete successfully. However, Kubernetes may still have the old working pods alongside the new broken ones, depending on the deployment strategy. Understanding this state is critical for choosing the right recovery approach.",
      "what_it_does": "Shows that the release is in FAILED state after the bad upgrade.",
      "next_step": "Examine the release history for forensics.",
      "cleanup": false
    },
    {
      "name": "Step 6: View Release History",
      "command": "helm history recovery-demo -n helm-scenarios",
      "description": "View the complete release history showing all revisions and their status",
      "explanation": "The history shows: Revision 1 (status: superseded) was the working deployment, and Revision 2 (status: failed) is the broken upgrade. This forensic information tells you which revision to roll back to. In production, you might have many revisions -- history helps you identify the last known-good state.",
      "what_it_does": "Lists all revisions with their status, timestamp, and description.",
      "next_step": "Examine the broken pods to understand the failure.",
      "cleanup": false
    },
    {
      "name": "Step 7: Diagnose Pod Failure",
      "command": "kubectl get pods -n helm-scenarios -l app.kubernetes.io/instance=recovery-demo && echo '---' && kubectl describe pod -n helm-scenarios -l version=v2-broken 2>/dev/null | grep -A 5 'Events:' || echo 'No v2-broken pods found (old pods may still be running)'",
      "description": "Examine pods to understand why the upgrade failed",
      "explanation": "You will see pods in ImagePullBackOff or ErrImagePull state for the bad image. The events section reveals the root cause: 'Failed to pull image nginx:99.99.99-nonexistent'. In production, this diagnostic step tells you whether the fix is a corrected image tag, registry credentials, or something else entirely.",
      "what_it_does": "Shows pod status and events explaining the failure cause.",
      "next_step": "Recover the release by rolling back to revision 1.",
      "cleanup": false
    },
    {
      "name": "Step 8: Rollback to Working Revision",
      "command": "helm rollback recovery-demo 1 -n helm-scenarios --wait --timeout 90s",
      "description": "Roll back to revision 1 (the known-good deployment)",
      "explanation": "Helm rollback restores the release to revision 1's configuration (nginx:1.24-alpine, 2 replicas). This creates a new revision (3) with the same configuration as revision 1. The --wait flag ensures the rollback completes and pods are healthy. This is the fastest and safest recovery method when a previous good state exists.",
      "what_it_does": "Restores the release to revision 1 configuration, creating revision 3.",
      "next_step": "Verify the rollback succeeded.",
      "cleanup": false
    },
    {
      "name": "Step 9: Verify Rollback Success",
      "command": "helm status recovery-demo -n helm-scenarios && echo '---' && helm history recovery-demo -n helm-scenarios && echo '---' && kubectl get pods -n helm-scenarios -l app.kubernetes.io/instance=recovery-demo",
      "description": "Confirm the release is back in 'deployed' state with healthy pods",
      "explanation": "The status should show 'deployed' again. The history now has 3 revisions: revision 1 (superseded), revision 2 (failed), revision 3 (deployed -- rollback to 1). All pods should be Running with nginx:1.24-alpine. The release is fully recovered.",
      "what_it_does": "Confirms the rollback restored a healthy deployment state.",
      "next_step": "Learn the alternative recovery method: uninstall and reinstall.",
      "cleanup": false
    },
    {
      "name": "Step 10: Alternative Recovery - Uninstall",
      "command": "helm uninstall recovery-demo -n helm-scenarios --wait",
      "description": "Demonstrate the alternative recovery: completely remove the release",
      "explanation": "Sometimes rollback is not possible (e.g., the first install itself failed, or the release is stuck in pending-install). In such cases, uninstalling and reinstalling is the cleanest recovery path. This removes the release and all its resources entirely, giving you a clean slate.",
      "what_it_does": "Removes the recovery-demo release completely.",
      "next_step": "Reinstall with known-good values.",
      "cleanup": false
    },
    {
      "name": "Step 11: Alternative Recovery - Reinstall",
      "command": "helm install recovery-demo helm-scenarios/10-failed-release-recovery/ -f helm-scenarios/10-failed-release-recovery/values.yaml -n helm-scenarios --wait --timeout 90s",
      "description": "Reinstall the chart fresh with working values",
      "explanation": "After uninstalling, we reinstall from scratch with known-good values. This starts fresh at revision 1. The tradeoff is that you lose all release history, but you get a completely clean state. In production, this approach is best when the release history is corrupted or the state is unrecoverable via rollback.",
      "what_it_does": "Reinstalls recovery-demo fresh with working configuration.",
      "next_step": "Verify the fresh installation is healthy.",
      "cleanup": false
    },
    {
      "name": "Step 12: Verify Fresh Installation",
      "command": "helm status recovery-demo -n helm-scenarios && echo '---' && helm history recovery-demo -n helm-scenarios",
      "description": "Confirm the fresh install is healthy with a clean history",
      "explanation": "The status shows 'deployed' and the history starts fresh with only revision 1. This confirms the uninstall-reinstall recovery was successful. You now understand both recovery strategies: rollback (preserves history, faster) and reinstall (clean slate, always works).",
      "what_it_does": "Confirms the fresh installation is healthy with a single revision.",
      "next_step": "Scenario complete! Clean up when ready.",
      "cleanup": false
    },
    {
      "name": "Cleanup: Remove Release",
      "command": "helm uninstall recovery-demo -n helm-scenarios --wait 2>/dev/null; kubectl delete pods -n helm-scenarios -l app.kubernetes.io/instance=recovery-demo --force --grace-period=0 2>/dev/null || true",
      "description": "Remove the recovery-demo release and any leftover pods",
      "explanation": "Uninstalls the Helm release and force-deletes any pods that might be stuck from the failed upgrade. The namespace is left intact for other scenarios.",
      "what_it_does": "Removes the 'recovery-demo' release and cleans up any stuck resources.",
      "next_step": "Cleanup complete!",
      "cleanup": true
    }
  ]
}
